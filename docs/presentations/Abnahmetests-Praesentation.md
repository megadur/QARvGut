# QARvGut MVP - Fachliche Abnahmetests
## PrÃ¤sentation fÃ¼r Stakeholder

---

## ğŸ“Š Agenda

1. **Ãœberblick Ã¼ber Testkonzept**
2. **Teil I: Fachliche Abnahmetests (BUC)**
3. **Teil II: End-to-End Integration Tests (E2E)**
4. **Test-Kategorien & AusfÃ¼hrungsstrategie**
5. **Definition of Done**
6. **NÃ¤chste Schritte**

---

## ğŸ¯ Warum Fachliche Abnahmetests?

### QualitÃ¤tssicherung auf GeschÃ¤ftsebene

- âœ… **Validierung der Anforderungen**: Jeder Test verifiziert eine konkrete GeschÃ¤ftsanforderung
- âœ… **Risikominderung**: FrÃ¼he Erkennung von Fehlern vor Produktiv-Deployment
- âœ… **Dokumentation**: Tests dienen als lebende Dokumentation des Systems
- âœ… **Kundenzufriedenheit**: Stellt sicher, dass die LÃ¶sung tatsÃ¤chlich den Bedarf erfÃ¼llt
- âœ… **Compliance**: Besonders wichtig fÃ¼r DSGVO und regulatorische Anforderungen

### Unser Ansatz

**Zweigleisiges Test-Konzept:**
1. **Isolierte Tests** â†’ Einzelne Use Cases getestet (Teil I)
2. **Integrierte Tests** â†’ Ãœbergreifende Workflows getestet (Teil II)

---

## ğŸ“‹ Teil I: Fachliche Abnahmetests

### Umfang & Struktur

| Kategorie | Anzahl Use Cases | TestfÃ¤lle | Fokus |
|-----------|-----------------|-----------|--------|
| **Sprint 1 - Kritisch** | 3 | 13 | Authentifizierung, Onboarding |
| **Sprint 2 - Hoch** | 5 | 43 | AuftrÃ¤ge, Dokumente, Synchronisation |
| **Sprint 3 - Mittel** | 3 | 23 | E-Mail, Datenaufbewahrung, Support |
| **Sprint 3+ - Niedrig** | 2 | 6 | Erweiterte Verwaltung |
| **GESAMT** | **13** | **85+** | |

### Business Use Cases (BUC)

#### Sprint 1 - Kritische Foundation
- **BUC-01** Gutachter-Onboarding (4 Tests)
- **BUC-02** System-Authentifizierung (5 Tests)
- **BUC-03** DRV-Mitarbeiter-Zugriffsverwaltung (4 Tests)

#### Sprint 2 - Kern-FunktionalitÃ¤t
- **BUC-04** AuftragsÃ¼bersicht & -verwaltung (6 Tests)
- **BUC-05** Auftragsdetails & Dokumenteneinsicht (8 Tests)
- **BUC-10** Automatische Dokumentenbereitstellung (7 Tests)
- **BUC-12a** Gutachter Ã¤ndern Auftragsstatus (6 Tests)
- **BUC-13** Auftragsstornierung (6 Tests)

#### Sprint 3 - GeschÃ¤ftsprozesse
- **BUC-06** E-Mail-Benachrichtigungssystem (6 Tests)
- **BUC-09** Datenaufbewahrung & DSGVO-LÃ¶schung (6 Tests)
- **BUC-11** StatusÃ¤nderungen Gutachter (7 Tests)

#### Sprint 3+ - Erweiterte Features
- **BUC-07** Support-Dashboard & Ãœberwachung (3 Tests)
- **BUC-08** Erweiterte Gutachtermitarbeiter-Verwaltung (3 Tests)

---

## ğŸ“ Testfall-Struktur (Beispiel: BUC-01)

### BUC-01: Gutachter-Onboarding-Prozess

| Element | Beschreibung |
|---------|-------------|
| **Anforderung ID** | BUC-01.01 - Erfolgreiche Gutachter-Registrierung |
| **Testfall ID** | TC-BUC01.1 |
| **Testfall-Beschreibung** | Kompletter Admin-verwalteter Registrierungsprozess |
| **Voraussetzungen** | eLogin/rvSMD verfÃ¼gbar, DRV-Mitarbeiter verfÃ¼gbar |
| **Testschritte** | 1. EFN in rvSMD eintragen<br>2. eLogin-Account erstellen<br>3. Daten Ã¼bertragen<br>4. Aktivierungscode senden<br>5. Account aktivieren |
| **Erwartetes Ergebnis** | Account aktiv, Login mÃ¶glich |
| **PrioritÃ¤t** | Hoch |

**ZusÃ¤tzlich fÃ¼r jeden Test:**
- ğŸ”´ Positive Tests (Erfolgsfall)
- ğŸŸ¡ Negative Tests (FehlerfÃ¤lle)
- ğŸŸ  Edge Cases (GrenzfÃ¤lle)
- ğŸ”µ Performance/Security Tests (bei kritischen Features)

---

## ğŸ”— Teil II: End-to-End Integration Tests

### Zweck & Unterschied zu Teil I

| Aspekt | Teil I (Fachlich) | Teil II (E2E) |
|--------|------------------|--------------|
| **Fokus** | Einzelne Use Cases | Ãœbergreifende Workflows |
| **Anzahl** | 85+ Tests | 5 Workflows |
| **KomplexitÃ¤t** | Isoliert | Multi-System Integration |
| **Timing** | FrÃ¼h im Projekt | Nach Einzel-Tests abgenommen |

### Die 5 E2E Workflows

#### ğŸš€ **E2E-01**: Kompletter Gutachter-Onboarding bis erste Anmeldung
```
Registrierung â†’ Admin-Genehmigung â†’ Account-Aktivierung â†’ Erste Anmeldung
â†’ Arbeitsbereitschaft (AuftragsÃ¼bersicht erreichbar)
```
âœ… Validiert: Kompletter Workflow ohne manuelle Eingriffe

#### ğŸ‘¨â€ğŸ’¼ **E2E-02**: DRV-Support-Workflow Komplettvalidierung
```
Support-Zugang â†’ Gutachter-Verwaltung â†’ Auftragszuweisung â†’ 
Monitoring & Reporting
```
âœ… Validiert: Support-Mitarbeiter kÃ¶nnen alle Tasks durchfÃ¼hren

#### ğŸ“‹ **E2E-03**: Gutachter Arbeitsalltag
```
Anmeldung â†’ AuftragsÃ¼bersicht â†’ Auftragsdetails Ã¶ffnen â†’ 
Dokumente einsehen â†’ Notizen erstellen â†’ Status Ã¤ndern â†’ 
Benachrichtigung erhalten
```
âœ… Validiert: TÃ¤glicher Arbeitsablauf ist produktiv

#### ğŸ”’ **E2E-04**: DSGVO-Compliance Lebenszyklus
```
Auftrag erstellen â†’ Bearbeiten â†’ AbschlieÃŸen â†’ 
(90 Tage) â†’ LÃ¶schbenachrichtigung â†’ Automatische LÃ¶schung
```
âœ… Validiert: Datenschutz ist implementiert

#### ğŸ‘¥ **E2E-05**: Multi-User Support-Szenario (mit Mitarbeitern)
```
Mitarbeiter registrieren â†’ DRV genehmigt â†’ Berechtigungen setzen â†’
Parallel-Bearbeitung â†’ Support-Monitoring
```
âœ… Validiert: Team-Arbeit ist mÃ¶glich

---

## ğŸ“Š Test-Kategorien & AusfÃ¼hrungsstrategie

### Test-Kategorien

| Kategorie | Tests | Frequenz | Automatisierung |
|-----------|-------|----------|-----------------|
| **Smoke Tests** | 5 Tests | Jeden Build | ğŸŸ¢ Hoch |
| **Regression Tests** | 80+ Tests | Vor Release | ğŸŸ¡ Mittel |
| **End-to-End Tests** | 5 Workflows | WÃ¶chentlich | ğŸ”´ Niedrig |
| **Performance Tests** | 3 Tests | GroÃŸe Ã„nderungen | ğŸŸ¡ Mittel |
| **Security Tests** | 3 Tests | Vor Release | ğŸ”´ Niedrig |
| **DSGVO-Compliance** | 6 Tests | Monatlich | ğŸ”´ Niedrig |

### Test-AusfÃ¼hrungs-Timeline

```
Development-Phase
â”œâ”€ Kontinuierliche Smoke Tests (automatisiert)
â”œâ”€ Unit & Integration Tests
â”‚
System-Integration Phase
â”œâ”€ Regression Tests (Fachliche Abnahmetests)
â”œâ”€ Performance Tests
â”‚
UAT-Phase (User Acceptance Testing)
â”œâ”€ End-to-End Tests
â”œâ”€ Security Tests
â”œâ”€ DSGVO-Compliance Validierung
â”‚
Production-Readiness
â””â”€ Finale Abnahmetests durch PO
```

---

## âœ… Definition of Done fÃ¼r Abnahmetests

Ein Use Case gilt als **fachlich abgenommen**, wenn:

### Funktionale Kriterien
- âœ… Alle Hauptszenarien erfolgreich getestet
- âœ… Mindestens 80% Alternativszenarien funktionieren
- âœ… GeschÃ¤ftsprozess-Tests vollstÃ¤ndig durchlaufen
- âœ… Alle kritischen Sicherheitsanforderungen erfÃ¼llt

### QualitÃ¤ts-Kriterien
- âœ… UI ist benutzerfreundlich & intuitiv
- âœ… Fehlermeldungen sind verstÃ¤ndlich
- âœ… Performance erfÃ¼llt Anforderungen
- âœ… Cross-Browser KompatibilitÃ¤t gegeben

### Dokumentation & Abnahme
- âœ… Test-Protokolle vollstÃ¤ndig
- âœ… Alle Befunde dokumentiert
- âœ… Screenshots/Videos fÃ¼r kritische Workflows
- âœ… **Product Owner hat explizit abgenommen âœï¸**

---

## ğŸ§ª Test-Beispiel Deep Dive

### BUC-04: AuftragsÃ¼bersicht und -verwaltung

**Kritische Szenarien:**
1. **Positive Test**: VollstÃ¤ndige AuftragsÃ¼bersicht laden
   - 50+ AuftrÃ¤ge gleichzeitig
   - Sortierung & Filterung funktioniert
   - Performance: < 3 Sekunden

2. **Negative Test**: Stornierte AuftrÃ¤ge anzeigen
   - Gekennzeichnet & gesperrt
   - Keine StatusÃ¤nderung mÃ¶glich

3. **Edge Case**: Keine AuftrÃ¤ge vorhanden
   - Hilfreiche Meldung statt Fehler

4. **Security Test**: Unbefugter Zugriff
   - Gutachter A kann AuftrÃ¤ge von B nicht sehen

5. **Performance Test**: 10 Gutachter gleichzeitig
   - Alle PDFs aus lokalem Cache
   - Response: < 200ms

---

## ğŸ“ˆ Testabdeckungs-Metriken

### Aktueller Status (Stand: 18. November 2025)

```
Business Use Cases:     13/13 âœ… (100%)
â”œâ”€ Sprint 1:           3/3 âœ…
â”œâ”€ Sprint 2:           5/5 âœ…
â”œâ”€ Sprint 3:           3/3 âœ…
â””â”€ Sprint 3+:          2/2 âœ…

Test Cases:            85+ âœ…
â”œâ”€ Positive Tests:     ~45
â”œâ”€ Negative Tests:     ~25
â”œâ”€ Edge Cases:         ~12
â””â”€ Performance/Security: ~6

End-to-End Workflows:  5/5 âœ…
```

### Risikoabdeckung

| Risikokategorie | Abdeckung | TestfÃ¤lle |
|-----------------|-----------|-----------|
| **Sicherheit** | â­â­â­â­â­ | 8+ Security Tests |
| **Datenschutz (DSGVO)** | â­â­â­â­â­ | 6 Compliance Tests |
| **Systemintegration** | â­â­â­â­â­ | 5 E2E Workflows |
| **Benutzerfreundlichkeit** | â­â­â­â­â˜† | 40+ UI Tests |
| **Performance** | â­â­â­â­â˜† | 8 Performance Tests |

---

## ğŸ”§ Testdaten & Test-Umgebung

### Verwendete Testdaten

```yaml
Gutachter:
  Max Mustermann:
    EFN: EFN123456789
    Email: max.mustermann@test-gutachter.de
    Status: Aktiv
  
  Anna Schmidt:
    EFN: EFN987654321
    Email: anna.schmidt@test-gutachter.de
    Status: Aktiv

DRV-Mitarbeiter:
  TestAdmin:
    Email: testadmin@drv-test.de
    Rolle: Support-Mitarbeiter
```

### Test-Umgebung

**Erforderliche Komponenten:**
- âœ… eLogin (Test-Instance)
- âœ… rvSMD (Schnittstelle)
- âœ… rvArchiv (Dokument-Storage)
- âœ… E-Mail System (SMTP Test)
- âœ… QARvGut Development/Staging
- âœ… Datenbank (Reset zwischen Tests)

---

## ğŸ“‹ Defekt-Management

### Defekt-Klassifizierung

| PrioritÃ¤t | Auswirkung | Beispiel |
|-----------|-----------|---------|
| ğŸ”´ **Kritisch** | System nicht benutzbar, Datenverlust | Kein Login mÃ¶glich |
| ğŸŸ  **Hoch** | Hauptfunktion nicht verfÃ¼gbar | AuftrÃ¤ge nicht sichtbar |
| ğŸŸ¡ **Mittel** | Nebenfunktion beeintrÃ¤chtigt | Filter funktioniert nicht |
| ğŸŸ¢ **Niedrig** | Kosmetische Probleme | Button-Farbe falsch |

### Defekt-Workflow

```
1. Defekt identifizieren & reproduzieren
2. Screenshot/Video als Beweismaterial
3. Im Tracking-System erfassen (ID, Beschreibung, PrioritÃ¤t)
4. Entwicklungsteam benachrichtigen
5. Fix verifizieren nach Implementierung
```

---

## ğŸ¬ Ablauf der TestausfÃ¼hrung

### Vor der TestausfÃ¼hrung
- âœ… Testumgebung konfiguriert
- âœ… Testdaten geladen
- âœ… Alle Systeme erreichbar
- âœ… Test-Browser vorbereitet

### WÃ¤hrend der TestausfÃ¼hrung
- âœ… Jeden Testfall einzeln durchfÃ¼hren
- âœ… Screenshots bei kritischen Schritten
- âœ… Abweichungen sofort dokumentieren
- âœ… Testdauer fÃ¼r Performance messen

### Nach der TestausfÃ¼hrung
- âœ… Testprotokolle vollstÃ¤ndig ausfÃ¼llen
- âœ… Defekte im Tracking-System erfassen
- âœ… Test-Artefakte archivieren
- âœ… Umgebung fÃ¼r nÃ¤chsten Lauf vorbereiten

---

## ğŸš€ NÃ¤chste Schritte

### Phase 1: Vorbereitung (2-3 Wochen)
- [ ] Test-Umgebung einrichten
- [ ] Testdaten-Sets erstellen
- [ ] Test-Automation Tools konfigurieren
- [ ] Team-Training durchfÃ¼hren

### Phase 2: TestausfÃ¼hrung (4-6 Wochen)
- [ ] Smoke Tests laufen lassen
- [ ] Regression Tests durchfÃ¼hren
- [ ] Defekte dokumentieren & fixen
- [ ] Performance-Tests validieren

### Phase 3: UAT (2-3 Wochen)
- [ ] End-to-End Tests durchfÃ¼hren
- [ ] Security-Tests validieren
- [ ] DSGVO-Compliance checken
- [ ] Finale PO-Abnahme

### Phase 4: Production-Release
- [ ] Abnahme durch Stakeholder
- [ ] Deployment-Vorbereitung
- [ ] Support-Training
- [ ] Go-Live

---

## ğŸ’¡ HÃ¤ufig gestellte Fragen

### F1: Warum 85+ TestfÃ¤lle?
**A:** Wir folgen dem Ansatz "Test Every Use Case":
- Positive Tests (Happy Path)
- Negative Tests (Fehlerszenarien)
- Edge Cases (GrenzfÃ¤lle)
- Performance & Security Tests

### F2: KÃ¶nnen Tests automatisiert werden?
**A:** Teilweise:
- Smoke & Regression Tests: ğŸŸ¢ 100% automatisierbar
- E2E Tests: ğŸŸ¡ 50-70% automatisierbar
- UI-intensive Tests: ğŸ”´ Manuell empfohlen

### F3: Wie lange dauert die TestausfÃ¼hrung?
**A:** 
- Smoke Tests: ~2 Stunden
- Regression Tests: ~30-40 Stunden
- E2E Tests: ~10-15 Stunden
- **Gesamt: ~50 Stunden (mit Team parallelisiert)**

### F4: Was ist, wenn ein Test fehlschlÃ¤gt?
**A:** 
1. Fehler dokumentieren
2. Bug im Tracking erfassen
3. Entwicklungsteam informieren
4. Regressions-Test durchfÃ¼hren
5. Test erneut ausfÃ¼hren

### F5: Wer fÃ¼hrt die Tests durch?
**A:** 
- **Smoke Tests**: CI/CD Pipeline (automatisch)
- **Regression Tests**: QA-Team (manuell + automatisiert)
- **E2E Tests**: QA-Team + Product Owner
- **UAT**: Fachexperten & Stakeholder

---

## ğŸ“š Dokumentation & Referenzen

### Verwandte Dokumente
- ğŸ“„ [Fachliche Abnahmetests MVP](fachliche-abnahmetests-mvp.md) - VollstÃ¤ndiger Testfall-Katalog
- ğŸ“„ [Use Cases MVP Development](use-cases-mvp-development.md) - Anforderungen
- ğŸ“„ [Project Status](PROJECT_STATUS.md) - Projekt-Status
- ğŸ“„ [Brownfield Architecture](brownfield-architecture/) - Technische Details

### Test-Metriken Repository
```
docs/tests/
â”œâ”€â”€ fachliche-abnahmetests-mvp.md  â† Alle 85+ Tests
â”œâ”€â”€ test-protokolle/               â† AusfÃ¼hrungs-Ergebnisse
â”œâ”€â”€ defect-tracking/               â† Gefundene Issues
â””â”€â”€ performance-reports/           â†’ Performance-Daten
```

---

## ğŸ“ Kontakt & Support

**Bei Fragen zu den Tests:**
- ğŸ“§ **Product Owner**: Sarah
- ğŸ‘¨â€ğŸ’» **QA-Leitung**: [Name QA]
- ğŸ› ï¸ **Tech Lead**: [Name Tech]

**Zeitplan fÃ¼r nÃ¤chstes Treffen:**
- ğŸ“… Detaillierte Testplan-Review
- ğŸ“… Test-Environment Setup
- ğŸ“… Team-Training DurchfÃ¼hrung

---

## âœ¨ Zusammenfassung

### Was wir erreicht haben:
âœ… **13 Business Use Cases** vollstÃ¤ndig definiert  
âœ… **85+ TestfÃ¤lle** fÃ¼r umfassende Abdeckung  
âœ… **5 E2E Workflows** fÃ¼r Integrationstests  
âœ… **Definition of Done** klar dokumentiert  
âœ… **TestausfÃ¼hrungsstrategie** etabliert  

### Was das bedeutet:
ğŸ¯ **Hohe QualitÃ¤tsstandards** fÃ¼r die LÃ¶sung  
ğŸ¯ **Risikominderung** durch frÃ¼he Fehlererkennung  
ğŸ¯ **Kundenzufriedenheit** durch validierte Features  
ğŸ¯ **Compliance & Sicherheit** gewÃ¤hrleistet  
ğŸ¯ **Schnellere Time-to-Market** durch klare Kriterien  

---

## ğŸ™ Fragen?

```
Vielen Dank fÃ¼r Ihre Aufmerksamkeit!

Wir sind bereit fÃ¼r die Test-Phase und nehmen 
alle Anforderungen ernst. Zusammen werden wir 
ein qualitativ hochwertiges System liefern.
```

**NÃ¤chster Schritt:** 
â†’ Test-Umgebung einrichten & Testlauf beginnen! ğŸš€
